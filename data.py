import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from keras.models import Sequential #empty model 
from keras.layers import Dense, Input
from keras.optimizers import Adam
from os import path
import gc

# Constants
VECTORIZED_DATA_FILE = "vectorized_data.csv"
DATASET_FILE = "dataset.csv"  # Assuming dataset filename

def load_or_create_vectorized_data():
    """Load or create vectorized data from source code."""
    if path.exists(VECTORIZED_DATA_FILE):
        print(f"Loading vectorized data from {VECTORIZED_DATA_FILE}...")
        vectorized_data_df = pd.read_csv(VECTORIZED_DATA_FILE)
        print("Dataset loaded and filtered successfully.")
        dataset = pd.read_csv(DATASET_FILE)
        filtered_dataset = dataset[["Vulnerability type"]]
        return vectorized_data_df, filtered_dataset["Vulnerability type"]
    else:
        print("Vectorizing source code...")
        X, y = preprocess_dataset()
        vectorized_data_df = vectorize_data(X)
        vectorized_data_df.to_csv(VECTORIZED_DATA_FILE, index=False)
        print(f"Vectorized data saved to {VECTORIZED_DATA_FILE}.")
        return vectorized_data_df, y

def preprocess_dataset():
    """Preprocess the dataset to get necessary features."""
    print("Loading dataset...")
    dataset = pd.read_csv(DATASET_FILE)
    filtered_dataset = dataset[["Vulnerability type", "Source code"]]
    print("Dataset loaded and filtered successfully.")
    return filtered_dataset["Source code"], filtered_dataset["Vulnerability type"]

def vectorize_data(X):
    """Convert a collection of text documents to a matrix of token counts."""
    vectorizer = CountVectorizer(max_features=1000)
    vectorizer.fit(X)
    X_bow = vectorizer.transform(X)
    print(f"Vocabulary built with {len(vectorizer.vocabulary_)} tokens.")
    return pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())


def build_logistic_regression_model(input_dim = 1000):
    """Build and compile a Keras model that mimics logistic regression."""

    model = Sequential()

    model.add(Dense(1, activation='sigmoid', input_dim=input_dim))

    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

    return model


def build_nn_model(number_layers = 3, first_layer = 512, number_nuerons = 32, input_dim = 1000):
    """Build and compile a Keras model that mimics logistic regression.
        3 layers
            1: 100 Nuerons
            2: 32 Nuerons: Linear activation
            3: 1 -> activation function: sigmoid (it is the last one and we want it to make a decision)
    """
    #number of layers and neurons in each! 
    model = Sequential() #layers are inserted in order  (but it is empty)
    model.add(Input(shape = (input_dim,)))
    model.add(Dense(first_layer, activation='relu')) #just for this layer!

    for _layer in range(number_layers):
        model.add(Dense(number_nuerons, activation='relu'))
    
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model



def test_nns(X_train, X_test, y_train, y_test):
    #make a for loop so that we test different types of models 
    for layer_number in [1, 3, 5, 10]:
        for num_nuerons in [8, 32, 124, 512]:
            for num_first_layer in [512, 1024]:

                nn_model = build_nn_model(layer_number, num_first_layer, num_nuerons)
                nn_model.fit(X_train, y_train)
                predictions = nn_model.predict(X_test)
                acc = _accuracy_score(predictions, y_test)
                print(f"Model config: Layers: {layer_number}, First Nuerons:{num_first_layer}, num_nueron_hidden: {num_nuerons} accuracy: {acc}")

                # Delete the model and collect garbage
                del nn_model
                gc.collect()

def _accuracy_score(predictions, y):
    _pred = (predictions >= .5).flatten() #(T or F)
    acc = sum(_pred == y) / len(y)

    return acc

def train_and_evaluate_model(X, y):
    """Split data, train model, and evaluate accuracy."""
    print("Training the model...")
    y_binary = y == "VULNERABLE"
    X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)
    test_nns(X_train, X_test, y_train, y_test)

    # model = build_logistic_regression_model()

# Main execution
X_bow_df, y = load_or_create_vectorized_data()
train_and_evaluate_model(X_bow_df, y)