import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from os import path

file_name = "dataset.csv"
vectorized_data_file = "vectorized_data.csv"

# Load the dataset
print("Loading dataset...")
dataset = pd.read_csv(file_name)
filtered_dataset = dataset[["Vulnerability type", "Source code", "Line", "Error type"]]
print("Dataset loaded successfully.")

# Process error types to get unique identifiers
print("Processing error types for unique identifiers...")
unique_error_types = filtered_dataset["Error type"].dropna().unique()
error_prefixes = set()
for error in unique_error_types:
    prefix = error.split(":")[0]
    error_prefixes.add(prefix)
print(f"Unique error prefixes extracted: {error_prefixes}")

# Decide whether to load or compute the vectorized data
if path.exists(vectorized_data_file):
    print(f"Loading vectorized data from {vectorized_data_file}...")
    X_bow = pd.read_csv(vectorized_data_file)
    print("Vectorized data loaded successfully.")
else:
    print("Vectorizing source code...")
    X = filtered_dataset["Source code"]
    vectorizer = CountVectorizer(max_features=1000)
    vectorizer.fit(X)  # This builds the vocabulary
    print(f"Vocabulary built with {len(vectorizer.vocabulary_)} tokens.")
    X_bow = vectorizer.transform(X)
    # Convert to DataFrame to save in CSV
    print("hi")
    X_bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())
    X_bow_df.to_csv(vectorized_data_file, index=False)
    print(f"Vectorized data saved to {vectorized_data_file}.")

# Example of using the data for further steps like splitting
y = filtered_dataset["Vulnerability type"] == "VULNERABLE"
X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)

lg = LogisticRegression()
lg.fit(X_train, y_train)
score = lg.score(X_test, y_test)
print(score)




    for layer_number in [1, 3, 5, 10]:
        for num_nuerons in [8, 32, 124, 512]:
            for num_first_layer in [512, 1024]:

                nn_model = build_nn_model(layer_number, num_first_layer, num_nuerons)
                nn_model.fit(X_train, y_train)
                predictions = nn_model.predict(X_test)
                acc = _accuracy_score(predictions, y_test)
                print(f"Model config: Layers: {layer_number}, First Nuerons:{num_first_layer}, num_nueron_hidden: {num_nuerons} accuracy: {acc}")